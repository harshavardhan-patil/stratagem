{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5804598a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import streamlit as st\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "import pdfplumber\n",
    "from langchain_community.chat_message_histories import StreamlitChatMessageHistory\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from src.data.rag import get_rich_case_studies\n",
    "\n",
    "# llm = ChatOllama(\n",
    "#     model='gemma3',\n",
    "#     temperature=0.1\n",
    "# )\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash-001\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=5,\n",
    "    max_retries=2,\n",
    "    # other params...\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a86ac84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def retrieve() -> str:\n",
    "    \"\"\"Retrieve context when you are asked for context\"\"\"\n",
    "    return \"Harsh is sleepy :)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "39fe60c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=ChatGoogleGenerativeAI(model='models/gemini-2.0-flash-001', google_api_key=SecretStr('**********'), temperature=0.0, max_retries=2, timeout=5.0, client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x718d49ab83e0>, default_metadata=()), kwargs={'tools': [{'type': 'function', 'function': {'name': 'retrieve', 'description': 'Retrieve context when you are asked for context', 'parameters': {'properties': {}, 'type': 'object'}}}]}, config={}, config_factories=[])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.bind_tools([retrieve])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6dda81bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = llm.invoke(\"Please give me context\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0a0c1c26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.tool_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "39106dbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Okay, I need some information from you to give you context!  Tell me:\\n\\n*   **What are you working on?** (e.g., writing a story, researching a topic, coding, having a conversation)\\n*   **What is the specific topic or subject you need context for?** (e.g., a historical event, a scientific concept, a piece of literature, a current event)\\n*   **What have you already tried or know about the topic?** (This helps me avoid repeating information you already have.)\\n*   **What kind of context are you looking for?** (e.g., historical background, definitions, explanations of related concepts, examples, different perspectives)\\n*   **What prompted you to ask for context?** (e.g., you encountered a term you didn't understand, you're trying to understand the significance of something, you're trying to explain something to someone else)\\n\\nThe more information you give me, the better I can understand what you need and provide helpful context.\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'model_name': 'gemini-2.0-flash-001', 'safety_ratings': []}, id='run-f9d76d41-f1b9-4d68-a0df-da6d92649700-0', usage_metadata={'input_tokens': 4, 'output_tokens': 218, 'total_tokens': 222, 'input_token_details': {'cache_read': 0}})"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25cb5083",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
